import theano
import theano.tensor as T
from theano.tensor.nnet.abstract_conv import bilinear_kernel_1D, bilinear_kernel_2D, conv2d_grad_wrt_inputs
import numpy as np

class ArgminUniqueOp(theano.Op):
	
	def __init__(self, unique, axis, min=True):
		self.unique = unique
		self.axis = axis
		self.min = min
	
	def make_node(self, x):
		x = theano.tensor.as_tensor_variable(x)
		inputs = [x]
		broadcastable = [b for i, b in enumerate(x.type.broadcastable) if i not in [self.axis]]
		outputs = [T.tensor('int64', broadcastable, name='argmax_unique')]
		return theano.Apply(self, inputs, outputs)
	
	def perform(self, node, inputs, outputs):
		x = inputs[0]
		z = outputs[0]
		z[0] = self._fill(x, self.unique, self.axis, self.min)
	
	def infer_shape(self, node, shapes):
		ishape = shapes[0]
		val = tuple([ishape[i] for (i, b) in enumerate(node.inputs[0].type.broadcastable) if i not in [self.axis]])
		return [val]

	def grad(self, inputs, grads):
		x, axis = inputs
		axis_grad = theano.gradient.grad_undefined(self, 1, axis, "ArgminUniqueOp doesn't have a gradient w.r.t. to its parameters.")
		return [x.zeros_like(), axis_grad]

	def _fill(self, x, unique, axis, min):
		dims = [d for i, d in enumerate(x.shape) if i != axis]
		indices = [np.arange(d, dtype=np.int32) for d in dims]
		indices = np.meshgrid(*indices)
		indices = np.concatenate([ind.reshape((-1,1)) for ind in indices], axis=1)
		indices = np.concatenate((indices[:,:axis], np.zeros((indices.shape[0],1), dtype=np.int32), indices[:,axis:]), axis=1)

		ret_val = np.zeros(dims, dtype=np.int64)

		dims = [d for i, d in enumerate(x.shape) if i not in [axis, unique]]
		taken_values = [[] for i in range(np.prod(dims))]
	
		if min:
			comp = lambda a,b: a < b
		else:
			comp = lambda a,b: b < a

		for i in range(indices.shape[0]):

			best_idx = -1
			best = np.inf
			if not min:
				best *= (-1)

			index = np.copy(indices[i,:])
			index_insert = np.delete(index, axis)
			index_flat = np.ravel_multi_index(
				np.delete(index, [axis, unique]),
				dims
			)

			for j in range(x.shape[axis]):
				index[axis] = j

				if comp(x[tuple(index)], best) and j not in taken_values[index_flat]:
					best_idx = j
					best = x[tuple(index)]

			# store value
			if best_idx > -1:
				taken_values[index_flat].append(best_idx)

			ret_val[tuple(index_insert)] = best_idx

		return ret_val
					

def argmin_unique(x, unique, axis, min=True):
	return ArgminUniqueOp(unique, axis, min)(x)

def bilinear_upsampling(input,
                        ratio,
                        batch_size=None,
                        num_input_channels=None,
                        use_1D_kernel=True):
    """Compute bilinear upsampling
    This function will build the symbolic graph for upsampling
    a tensor by the given ratio using bilinear interpolation.
    Parameters
    ----------
    input: symbolic 4D tensor
        mini-batch of feature map stacks, of shape (batch size,
        input channels, input rows, input columns) that will be upsampled.
    ratio: `int or Constant or Scalar Tensor of int* dtype`
        the ratio by which the input is upsampled in the 2D space (row and
        col size).
    batch_size: None, int or Constant variable
        The size of the first dimension of the input variable.
        Optional, possibly used to choose an optimal implementation.
        batch_size will be used only if num_input_channels is not None.
    num_input_channels: None, int or Constant variable
        The size of the second dimension of the input variable.
        Optional, possibly used to choose an optimal implementation.
        num_input_channels will be used only if batch_size is not None.
    use_1D_kernel: bool
        if set to true, row and column will be upsampled seperately by 1D
        kernels, otherwise they are upsampled together using a 2D kernel. The
        final result is the same, only the speed can differ, given factors such
        as upsampling ratio.
    Returns
    -------
    symbolic 4D tensor
        set of feature maps generated by bilinear upsampling. Tensor
        is of shape (batch size, num_input_channels, input row size * ratio,
        input column size * ratio)
    Notes
    -----
    :note: The kernel used for bilinear interpolation is fixed (not learned).
    :note: When the upsampling ratio is even, the last row and column is
        repeated one extra time compared to the first row and column which makes
        the upsampled tensor asymmetrical on both sides. This does not happen when
        the upsampling ratio is odd.
    """

    T = theano.tensor
    try:
        up_bs = batch_size * num_input_channels
    except TypeError:
        up_bs = None
    row, col = input.shape[2:]
    up_input = input.reshape((-1, 1, row, col))

    # concatenating the first and last row and column
    # first and last row
    concat_mat = T.concatenate((up_input[:, :, :1, :], up_input,
                                up_input[:, :, -1:, :]), axis=2)
    # first and last col
    concat_mat = T.concatenate((concat_mat[:, :, :, :1], concat_mat,
                                concat_mat[:, :, :, -1:]), axis=3)
    concat_col = col + 2

    pad = 2 * ratio - (ratio - 1) // 2 - 1

    if use_1D_kernel:
        kern = bilinear_kernel_1D(ratio=ratio, normalize=True)
        # upsampling rows
        upsampled_row = conv2d_grad_wrt_inputs(output_grad=concat_mat,
                                               filters=kern[np.newaxis,
                                                            np.newaxis, :,
                                                            np.newaxis],
                                               input_shape=(up_bs, 1,
                                                            row * ratio,
                                                            concat_col),
                                               filter_shape=(1, 1, None, 1),
                                               border_mode=(pad, 0),
                                               subsample=(ratio, 1),
                                               filter_flip=True)
        # upsampling cols
        upsampled_mat = conv2d_grad_wrt_inputs(output_grad=upsampled_row,
                                               filters=kern[np.newaxis,
                                                            np.newaxis,
                                                            np.newaxis, :],
                                               input_shape=(up_bs, 1,
                                                            row * ratio,
                                                            col * ratio),
                                               filter_shape=(1, 1, 1, None),
                                               border_mode=(0, pad),
                                               subsample=(1, ratio),
                                               filter_flip=True)
    else:
        kern = bilinear_kernel_2D(ratio=ratio, normalize=True)
        upsampled_mat = conv2d_grad_wrt_inputs(output_grad=concat_mat,
                                               filters=kern[np.newaxis,
                                                            np.newaxis, :, :],
                                               input_shape=(up_bs, 1,
                                                            row * ratio,
                                                            col * ratio),
                                               filter_shape=(1, 1, None, None),
                                               border_mode=(pad, pad),
                                               subsample=(ratio, ratio),
                                               filter_flip=True)

    return upsampled_mat.reshape((input.shape[0], input.shape[1],
                                  row * ratio, col * ratio))
